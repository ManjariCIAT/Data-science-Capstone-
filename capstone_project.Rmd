---
title: "Data Science Capstone - Milestone Report"
author: "Manjari Singh"
date: "2024-09-16"
output: html_document
---

##Synopsis
This report provides a Milestone Report for the Coursera Data Science Specialization Capstone Project. The goal of the capstone project is to work with data, create a predictive text model using a large text corpus as training data, in order to be able to predict subsequent words given some text. This will eventually be built as a Shiny application.

The motivation for this project is to: 
-Demonstrate that data have been downloaded and have successfully loaded it R.
-Create a basic report of summary statistics about the data sets.
-Report any interesting findings that you amassed so far


##Load necessary libraries
```{r libraries,warning=FALSE,message=FALSE}
#install.packages("stringi")
library(stringi)
#install.packages("tm")
library(tm)
#install.packages("SnowballC")
library(SnowballC)
#install.packages("RWeka")
library(RWeka)
library(ggplot2)
#install.packages("wordcloud")
library(wordcloud)
```

##Getting the Data

####Download Data

The dataset is downloaded from the following url: [Capstone Dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).
```{r download}
#if (!file.exists("Coursera-SwiftKey.zip")) {
#   download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-Sw#iftKey.zip","Coursera-SwiftKey.zip")
#   unzip("Coursera-SwiftKey.zip")
#}
```

```{r}
download.file("http://www.cs.cmu.edu/~biglou/resources/bad-words.txt","bad-words.txt")
```

#### Loading data into R

```{r loading,cache=TRUE}
setwd("C:/Users/MSingh/OneDrive - CGIAR/Documents/myFirstApp/Newfolder/Coursera-SwiftKey/final/en_US")

twitter_url <- "en_US.twitter.txt"
blog_url <- "en_US.blogs.txt"
twitter <- readLines(twitter_url, skipNul = TRUE, encoding = "UTF-8")
blog <- readLines(blog_url, skipNul = TRUE, encoding = "UTF-8")

# In order to bypass an "End Of File" error that appeared in the middle of the document (news), there is a different method of loading the file
news_url <- "en_US.news.txt"
news_file <- file(news_url,"rb")
news <- readLines(news_file, skipNul = TRUE, encoding = "UTF-8")
close(news_file)
```

##Basic Summary of Data

```{r summary,echo=TRUE,cache=TRUE}
create_summary_table <- function(twitter,blog,news){
  stats <- data.frame(source = c("twitter","blog","news"),
            arraySizeMB = c(object.size(twitter)/1024^2,object.size(blog)/1024^2,object.size(news)/1024^2),
            fileSizeMB = c(file.info(twitter_url)$size/1024^2,file.info(blog_url)$size/1024^2,file.info(news_url)$size/1024^2),
            lineCount = c(length(twitter),length(blog),length(news)),
            wordCount = c(sum(stri_count_words(twitter)),sum(stri_count_words(blog)),sum(stri_count_words(news))),
            charCount = c(stri_stats_general(twitter)[3],stri_stats_general(blog)[3],stri_stats_general(news)[3])
  )
  print(stats)
}
create_summary_table(twitter,blog,news)
```

##Sampling the data

since current dataset is quite large in size, sample 10,000 rows of each dataset,and then combine them into one
```{r sampling,cache=TRUE}
set.seed(1805)
sampleData <- c(sample(twitter,10000),sample(blog,10000),sample(news,10000))
```

##Cleaning the Data
for exploratory analysis, remove stopwords from the text. Retain these for model prediction
```{r cleaning,cache=TRUE}
corpus <- VCorpus(VectorSource(sampleData))

toSpace <- content_transformer(function(x, pattern) {return (gsub(pattern," ",x))})
#Cleaning all non ASCII characters
corpus <- tm_map(corpus,toSpace,"[^[:graph:]]")
#Transforming all data to lower case
corpus <- tm_map(corpus,content_transformer(tolower))
#Deleting all English stopwords and any stray letters left my the non-ASCII removal
corpus <- tm_map(corpus,removeWords,c(stopwords("english"),letters))
#Removing Punctuation
corpus <- tm_map(corpus,removePunctuation)
#Removing Numbers
corpus <- tm_map(corpus,removeNumbers)
#Removing Profanities
profanities = readLines('bad-words.txt')
corpus <- tm_map(corpus, removeWords, profanities)
#Removing all stray letters left by the last two calls
corpus <- tm_map(corpus,removeWords,letters)
#Striping all extra whitespace
corpus <- tm_map(corpus,stripWhitespace)
```

##Exploratory Analysis

create n-gram matrices for n=1,2,3. This will present the most frequent terms in the dataset

####Creating N-grams

```{r ngrams,cache=TRUE}
#Creating a unigram DTM
unigramTokenizer <- function(x) {NGramTokenizer(x, Weka_control(min = 1, max = 1))}
unigrams <- DocumentTermMatrix(corpus, control = list(tokenize = unigramTokenizer))

#Creating a bigram DTM
BigramTokenizer <- function(x) {NGramTokenizer(x, Weka_control(min = 2, max = 2))}
bigrams <- DocumentTermMatrix(corpus, control = list(tokenize = BigramTokenizer))

#Creating a trigram DTM
TrigramTokenizer <- function(x) {NGramTokenizer(x, Weka_control(min = 3, max = 3))}
trigrams <- DocumentTermMatrix(corpus, control = list(tokenize = TrigramTokenizer))
```

####Most Frequent Terms per N-gram

Below the top n-grams for n=1,2,3 can be seen.

```{r freqs}
freqTerms <- findFreqTerms(unigrams,lowfreq = 1000)
unigrams_frequency <- sort(colSums(as.matrix(unigrams[,freqTerms])),decreasing = TRUE)
unigrams_freq_df <- data.frame(word = names(unigrams_frequency), frequency = unigrams_frequency)
wordcloud(unigrams_freq_df$word,unigrams_freq_df$frequency,scale=c(4,.1), colors = brewer.pal(7, "Dark2"), random.order = TRUE, random.color = TRUE, rot.per = 0.35)

freqTerms <- findFreqTerms(bigrams,lowfreq = 75)
bigrams_frequency <- sort(colSums(as.matrix(bigrams[,freqTerms])),decreasing = TRUE)
bigrams_freq_df <- data.frame(word = names(bigrams_frequency), frequency = bigrams_frequency)
wordcloud(bigrams_freq_df$word,bigrams_freq_df$frequency,scale=c(3,.1), colors = brewer.pal(7, "Dark2"), random.order = TRUE, random.color = TRUE, rot.per = 0.35)

freqTerms <- findFreqTerms(trigrams,lowfreq = 10)
trigrams_frequency <- sort(colSums(as.matrix(trigrams[,freqTerms])),decreasing = TRUE)
trigrams_freq_df <- data.frame(word = names(trigrams_frequency), frequency = trigrams_frequency)
wordcloud(trigrams_freq_df$word,trigrams_freq_df$frequency,scale=c(3,.1), colors = brewer.pal(7, "Dark2"), random.order = TRUE, random.color = TRUE, rot.per = 0.35)
```

##Graphs

Below the the graphs for the most common ngrams can be seen.

####Most common unigrams
```{r}
g <- ggplot(unigrams_freq_df,aes(x=reorder(word,-frequency),y=frequency))+geom_bar(stat="identity",fill="darkolivegreen4") + xlab("Unigram") + ylab("Frequency") +labs(title="Most common unigrams") + theme(axis.text.x=element_text(angle=55, hjust=1))
g
```

####Most common bigrams
```{r}
g <- ggplot(bigrams_freq_df,aes(x=reorder(word,-frequency),y=frequency))+geom_bar(stat="identity",fill="darkolivegreen4") + xlab("Bigram") + ylab("Frequency") +labs(title="Most common bigrams") + theme(axis.text.x=element_text(angle=55, hjust=1))
g
```

####Most common trigrams
```{r}
g <- ggplot(trigrams_freq_df,aes(x=reorder(word,-frequency),y=frequency))+geom_bar(stat="identity",fill="darkolivegreen4") + xlab("Trigram") + ylab("Frequency") +labs(title="Most common trigrams") + theme(axis.text.x=element_text(angle=55, hjust=1))
g
```

##Prediction Algorithm and Shiny App

Concluding the exploratory analysis of the data, the next step of this project is to finalize the predictive algorithm, deploy the model as a Shiny application and also create a deck to be able to present the final result.

The predictive algorithm will be using an n-gram backoff model, where it will start by looking for the most common 3-gram or 4-gram that includes the provided text, and either choose the most common one based on frequency, or revert to the immediate smaller n-gram all the way to the unigram. The model will be trained on a bigger dataset than the one used for our exploratory data analysis, and it will include a suggestion based on the most common unigrams (with smoothed probabilities) in case no bigger n-gram provides a suggestion.